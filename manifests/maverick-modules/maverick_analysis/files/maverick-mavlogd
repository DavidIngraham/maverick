#!/usr/bin/env python
#
# Script to import ArduPilot logs into influxdb for analysis/display
# https://github.com/fnoop/maverick

import sys, os, shutil, signal, datetime, time, logging, errno, re, ConfigParser, traceback, subprocess, glob
import lockfile, signal, grp
import asyncore, pyinotify
import sqlite3
from grafana_api_client import GrafanaClient
from grafanalib.core import *
from daemon import runner
from pyinotify import WatchManager, AsyncNotifier, ThreadedNotifier, ProcessEvent, IN_CLOSE_WRITE
from pymavlink import DFReader
from pymavlink.DFReader import DFReader, DFReader_binary, DFReader_text
from influxdb import InfluxDBClient, SeriesHelper

class ProcessLog(ProcessEvent):
    def __init__(self, app):
        logger.debug("Starting ProcessLog.__init__")
        logger.debug("App influx_database:" + app.influx_database)
        self.app = app
        
    def process_IN_CLOSE_WRITE(self, event):
        logger.debug("Received IN_CLOSE_WRITE event")
        self.process(event.pathname)
    def process_IN_MOVED_TO(self, event):
        logger.debug("Received IN_MOVED_TO event")
        self.process(event.pathname)

    def process(self, filename):
        logger.info("Processing data from "+filename)
        if filename.endswith('.log'):
            log = DFReader_text(filename, False)
            logger.debug("Text mavlog detected, creating text parser")
        else:
            log = DFReader_binary(filename, False)
            logger.debug("Binary mavlog detected, creating binary parser")
        parameters = {}
        formats = {}
        json_points = []
        counter = 0
        first_timestamp = None
        first_epoch = None
        last_timestamp = None
        last_epoch = None
        while True:
            entry = log.recv_msg()
            if entry is None:
                logger.debug("No more log entries, break from processing loop")
                break
            msgtype = entry.fmt.name
            if msgtype == "PARM":
                # parameters[entry.Name] = entry.Value
                pass
            elif msgtype == "FMT":
                formats[entry.Name] = {'format': list(entry.Format), 'columns': entry.Columns.split(",")}
            elif msgtype == "MSG":
                logger.debug(" - Mavlink Message:"+str(entry.TimeUS)+":"+str(entry.Message))
            else:
                if msgtype in formats:
                    _fields = {column:getattr(entry, column) for column in formats[msgtype]['columns']}
                    last_timestamp = datetime.datetime.fromtimestamp(entry._timestamp).strftime('%Y-%m-%d %H:%M:%S.%fZ')
                    last_epoch = int(float(entry._timestamp)*1000000000)
                    # logger.debug("_timestamp:"+str(last_epoch)+":, last_timestamp:"+str(last_timestamp))
                    if not first_timestamp:
                        first_timestamp = last_timestamp
                        first_epoch = last_epoch
                    json_body = {
                        "database": self.app.influx_database,
                        "time_precision": "ns",
                        "measurement": "flight_"+msgtype.lower(),
                        "tags": {
                            "filename": filename
                        },
                        "time": last_epoch,
                        "fields": _fields
                    }
                    json_points.append(json_body)
                    counter += 1
            # Batch writes to influxdb, much faster
            if counter > 0 and counter % 1000 == 0:
                # logger.debug(" - Batch sending 1000 points to influxdb:"+str(counter))
                self.app.client.write_points(json_points)
                json_points = [] # Clear out json_points after bulk write
        # Write any remaining points
        self.app.client.write_points(json_points)
        logger.info("Processing complete of "+filename)
        logger.info(str(counter) + " datapoints inserted.  First timestamp is " +str(first_timestamp)+":"+str(first_epoch) + ", Last timestamp is " + str(last_timestamp)+":"+str(last_epoch))
        self.meta_entry(filename, int(first_epoch/1000000000), int(last_epoch/1000000000))
        self.archive_file(filename)
        self.dashboard_links()
    
    def meta_entry(self, filename, start, finish):
        # Work out source
        file = os.path.basename(filename)
        dirs = os.path.split(filename)
        dirfrags = dirs[0].split(os.path.sep)
        filename = dirs[1]
        # Connect to meta db
        conn = sqlite3.connect(self.app.config['meta_database'])
        cursor = conn.cursor()
        # See if entry already exists
        cursor.execute("SELECT id FROM logfiles WHERE filename='"+filename+"'")
        try:
            id_exists = cursor.fetchone()[0]
        except:
            id_exists = None
        if not id_exists:
            cursor.execute('INSERT INTO logfiles (filename, start, finish, source) VALUES (?,?,?,?)', (filename, start, finish, dirfrags[-1].title()))
        else:
            cursor.execute('UPDATE logfiles SET start=?,finish=? WHERE id=?', (start,finish,id_exists))
        conn.commit()
        conn.close()
        
    def archive_file(self, filename):
        file = os.path.basename(filename)
        dirs = os.path.split(filename)
        dirfrags = dirs[0].split(os.path.sep)
        destfile = os.path.sep.join(["/srv/maverick/data/mavlink/archive",dirfrags[-1],dirs[1]])
        shutil.move(filename, destfile)
        logger.info("Archiving file from " +filename+ " to " +destfile)

    def dashboard_links(self):
        # Retrieve metadata
        logentries = []
        conn = sqlite3.connect(self.app.config['meta_database'])
        cursor = conn.cursor()
        cursor.execute("SELECT filename,start,finish,source FROM logfiles")
        rows = cursor.fetchall()
        for row in rows:
            fromdt=str(int(float(row[1]))*1000)
            todt=str(int(float(row[2]))*1000)
            logentries.append("<tr><td>"+row[3]+"</td><td>"+row[0]+"</td><td>"+datetime.datetime.fromtimestamp(float(row[1])).strftime('%Y-%m-%d %H:%M:%S')+"</td><td>"+datetime.datetime.fromtimestamp(float(row[2])).strftime('%Y-%m-%d %H:%M:%S')+"</td><td><a href='/dashboard/db/flight-data-analysis?orgId=10&from="+fromdt+"&to="+todt+"'>Flight Data Analysis</a></td><td><a href='/dashboard/db/flight-ekf2-analysis?orgId=10&from="+fromdt+"&to="+todt+"'>EKF2 Analysis</a></td></tr>")
        conn.close()
        # Construct the dashboard
        dashboard = {
            "editable": False,
            "hideControls": True,
            "rows": [{"panels": [{
                  "content": "<table><thead><tr><th>Source</th><th>Filename</th><th>Start</th><th>Finish</th><th>Flight Data Link</th><th>EKF Data Link</th></tr></thead><tbody>"+"\n".join(logentries)+"</tbody></table>",
                  "mode": "html",
                  "span": 12,
                  "title": "Flight Logs",
                  "type": "text"
                }],
            "title": "Flight Logs",
            }],
            "schemaVersion": 14,
            "style": "dark",
            "timezone": "browser",
            "version": int(time.time()),
            "title": "Flight Logs Index"
        }
        # logger.debug("Dashboard:"+str(dashboard))
        # Create/update the dashboard in grafana
        grafana = GrafanaClient((self.app.config['grafana_user'], self.app.config['grafana_password']), host="127.0.0.1", port=6790)
        dashreturn = grafana.dashboards.db.create(dashboard=dashboard, overwrite=True)
        logger.debug("Grafana dashboard update:"+str(dashreturn))


### Main App Class
class App():
    def __init__(self):
        self.stdin_path = '/dev/null'
        self.stdout_path = '/srv/maverick/var/log/analysis/maverick-mavlogd.daemon.log'
        self.stderr_path = '/srv/maverick/var/log/analysis/maverick-mavlogd.daemon.log'
        self.pidfile_path = '/srv/maverick/var/run/maverick-mavlogd.pid'
        self.pidfile_timeout = 5
        
    def run(self):
        logger.info("")
        logger.info("--------------------------------")
        logger.info("Starting maverick-mavlogd daemon")
        # Get config
        self.config = self.config_parse("/srv/maverick/data/config/analysis/maverick-mavlogd.conf")
        self.influx_database = self.config['influx_database']
        self.client = InfluxDBClient(self.config['influx_server'], self.config['influx_port'], self.config['influx_user'], self.config['influx_password'], self.influx_database)
        self.client.create_database(self.influx_database)
        self.logdirs = self.config['logdirs']
        self.watch_dirs()
        self.metadb(self.config['meta_database'])
        # Loop and wait for files
        while True:
            time.sleep(1)

    def watch_dirs(self):
    	watch_manager = WatchManager()
    	notifier = ThreadedNotifier(watch_manager, default_proc_fun=ProcessLog(self))
        notifier.start()
    	for logdir in self.logdirs.split(","):
    		logger.info("Watching directory: "+str(logdir))
    		mask = pyinotify.IN_CLOSE_WRITE | pyinotify.IN_MOVED_TO | pyinotify.IN_MOVED_FROM
    		watch_manager.add_watch(logdir, mask)

    def config_parse(self, path):
        Config = ConfigParser.SafeConfigParser()
        options = {}
        try:
            Config.read(path)
            for item in Config.options("mavlogd"):
                options[item] = Config.get("mavlogd", item)
            logger.info("Config read from " + str(path))
        except:
            logger.critical("Error reading config file: " + str(path))
            sys.exit(1)
        return options

    def metadb(self, path):
        conn = sqlite3.connect(path)
        cursor = conn.cursor()
        cursor.execute('CREATE TABLE IF NOT EXISTS logfiles (id INTEGER PRIMARY KEY AUTOINCREMENT, filename TEXT, start TEXT, finish TEXT, source TEXT)')
        conn.commit()
        conn.close()

if __name__ == "__main__":
    # Setup logging
    logger = logging.getLogger("DaemonLog")
    logger.setLevel(logging.DEBUG)
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
    handler = logging.FileHandler("/srv/maverick/var/log/analysis/maverick-mavlogd.log")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    
    # Create app instance and daemonize
    app = App()
    daemon_runner = runner.DaemonRunner(app)
    daemon_runner.daemon_context.files_preserve=[handler.stream]
    try:
        daemon_runner.do_action()
    except runner.DaemonRunnerStopFailureError as Error:
        print " - mavlogd not running, cannot stop"
        # If stop action, just ignore the error and exit
        if daemon_runner.action == "stop":
            sys.exit(0)
        elif daemon_runner.action == "restart":
            print " - starting mavlogd"
            daemon_runner.action = "start"
            daemon_runner.do_action()
